"""MCP tool: search_documents."""

from __future__ import annotations

from fastmcp import Context
from opentelemetry import trace
from pydantic import Field
from mcp.shared.exceptions import McpError

from mcp_instance import mcp

from .opensearch_services import get_services
from .utils import (
    ToolResult,
    ctx_error,
    ctx_info,
    ctx_progress,
    mcp_internal_error,
    require_any_env_var,
    tool_result_text,
)

tracer = trace.get_tracer(__name__)


@mcp.tool(
    name="search_documents",
    description=(
        "–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ –ø–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. "
        "–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
    ),
)
async def search_documents(
    query: str = Field(..., description="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å."),
    index_name: str | None = Field(default=None, description="–ò–Ω–¥–µ–∫—Å OpenSearch –¥–ª—è –ø–æ–∏—Å–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)."),
    max_results: int = Field(default=10, ge=1, le=50, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (1-50)."),
    use_hyde: bool = Field(default=False, description="–í–∫–ª—é—á–∏—Ç—å HyDE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞."),
    use_colbert: bool = Field(default=True, description="–í–∫–ª—é—á–∏—Ç—å ColBERT —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥."),
    ctx: Context | None = None,
) -> ToolResult:
    with tracer.start_as_current_span("search_documents") as span:
        span.set_attribute("query_length", len(query))
        span.set_attribute("index_name", index_name or "")
        span.set_attribute("max_results", max_results)
        span.set_attribute("use_hyde", use_hyde)
        span.set_attribute("use_colbert", use_colbert)

        await ctx_info(ctx, "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        await ctx_progress(ctx, 0)

        try:
            require_any_env_var(["CLOUDRU_API_KEY", "API_KEY"])

            _, search_service, _ = get_services()
            await ctx_info(ctx, "üîé –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫")
            await ctx_progress(ctx, 50)

            documents = await search_service.search_documents(
                query=query,
                size=max_results,
                semantic_weight=0.7,
                keyword_weight=0.3,
                use_hyde=use_hyde,
                use_colbert=use_colbert,
                index_name=index_name,
            )

            await ctx_progress(ctx, 100)
            await ctx_info(ctx, "‚úÖ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω")

            span.set_attribute("results_count", len(documents))

            lines: list[str] = [f"–ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}", ""]
            for i, doc in enumerate(documents[: max_results], 1):
                source = doc.get("source", "unknown")
                chunk_id = doc.get("chunk_id", "")
                score = doc.get("_score", 0) or 0
                text = (doc.get("text") or "").strip()
                snippet = (text[:400] + "...") if len(text) > 400 else text
                lines.append(f"{i}. [{source}::{chunk_id}] (score: {score:.2f})")
                lines.append(snippet)
                lines.append("")

            return tool_result_text(
                "\n".join(lines).strip(),
                structured_content={"documents": documents, "total": len(documents)},
                meta={"tool": "search_documents"},
            )
        except McpError:
            raise
        except Exception as e:
            await ctx_error(ctx, f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            mcp_internal_error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫: {e}")
